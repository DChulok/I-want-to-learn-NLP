{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import coNLL.conll as co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'coNLL.conll' from '/Users/danilculkov/Desktop/Учеба/Кафедра/2020/I-want-to-learn-NLP/CoNLL-model/coNLL/conll.py'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll = co.CoNLL('./coNLL/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\n",
      "[['CRICKET', 'O'], ['-', 'O'], ['LEICESTERSHIRE', 'B-ORG'], ['TAKE', 'O'], ['OVER', 'O'], ['AT', 'O'], ['TOP', 'O'], ['AFTER', 'O'], ['INNINGS', 'O'], ['VICTORY', 'O'], ['.', 'O']]\n",
      "[['SOCCER', 'O'], ['-', 'O'], ['JAPAN', 'B-LOC'], ['GET', 'O'], ['LUCKY', 'O'], ['WIN', 'O'], [',', 'O'], ['CHINA', 'B-PER'], ['IN', 'O'], ['SURPRISE', 'O'], ['DEFEAT', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "for typ in conll.types:\n",
    "    conll.split_text_label(typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll.create_tag2idx('./coNLL/tag2idx.json')\n",
    "conll.create_idx2tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll.create_one_labeled_data('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnYk4xX4k37T"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22273,
     "status": "ok",
     "timestamp": 1607959613899,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "dg8fjZSik8Pq",
    "outputId": "e962eb0f-aade-487b-fc5e-625f3647d975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMs4kgHhk37T"
   },
   "source": [
    "Let's prepare the data for the input of BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 22261,
     "status": "ok",
     "timestamp": 1607959613900,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "3cwhATu8k37T"
   },
   "outputs": [],
   "source": [
    "def split_text_label(filename):\n",
    "    f = open(filename)\n",
    "    split_labeled_text = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                split_labeled_text.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
    "    if len(sentence) > 0:\n",
    "        split_labeled_text.append(sentence)\n",
    "        sentence = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for sent in split_labeled_text:\n",
    "        sentence = []\n",
    "        label = []\n",
    "        for s_l in sent:\n",
    "            sentence.append(s_l[0])\n",
    "            label.append(s_l[1])\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 24839,
     "status": "ok",
     "timestamp": 1607959616482,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "QkJDjFLLk37U"
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = split_text_label(\"coNLL/train.txt\")\n",
    "valid_data, valid_labels = split_text_label(\"coNLL/valid.txt\")\n",
    "test_data, test_labels = split_text_label(\"coNLL/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24838,
     "status": "ok",
     "timestamp": 1607959616484,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "zpfkVs3uk37U",
    "outputId": "55fe8067-aad7-4599-985c-d9aeebe964e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first sentence\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2ZTcTd7g0sj"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24334,
     "status": "ok",
     "timestamp": 1607959616485,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "vqh88Mq_k37U",
    "outputId": "7ff3c284-8b9a-4ad3-ca30-27039c9c6bd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# its tokens\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xLtWs2Bk37U"
   },
   "source": [
    "Lets create tag dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1475,
     "status": "ok",
     "timestamp": 1607959619338,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "qLS5BsOAk37U"
   },
   "outputs": [],
   "source": [
    "tag_values = set()\n",
    "for l in train_labels:\n",
    "    tag_values.update(l)\n",
    "tag_values.update([\"PAD\"])\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1607959622502,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "E8hopRdpk37U",
    "outputId": "e57e6ccb-cc9e-4abc-cddf-54e0170c2379"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " 'I-PER': 1,\n",
       " 'B-ORG': 2,\n",
       " 'I-MISC': 3,\n",
       " 'B-PER': 4,\n",
       " 'B-LOC': 5,\n",
       " 'O': 6,\n",
       " 'I-LOC': 7,\n",
       " 'I-ORG': 8,\n",
       " 'B-MISC': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1607959623859,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "3tpcTa6pk37V"
   },
   "outputs": [],
   "source": [
    "idx2tag = {v: k for k, v in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXAiaxejk37V"
   },
   "source": [
    "### Tokenization with BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Fig7rJPk37V"
   },
   "source": [
    "BERT (Bidirectional Encoder Representations from Transformers) is a method of pretraining language representations. These vectors (representations) are used as high-quality feature inputs to downstream models. BERT offers an advantage over models like Word2Vec, because while each word has a fixed representation under Word2Vec regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9225,
     "status": "ok",
     "timestamp": 1607959632766,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "peuZju_RRzWM",
    "outputId": "984df4c3-9cb5-4f83-e5c0-719a6d2ef4a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 10.6MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 40.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 45.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=59e8b106c7c0f5ce163cb71e89613d25b38cb4db54b7336164795683cbd91dec\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14744,
     "status": "ok",
     "timestamp": 1607959640034,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "neXo6aaGk37V",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 13936,
     "status": "ok",
     "timestamp": 1607959640036,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "7y0i13Nbk37V"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPqO5Tjsk37V"
   },
   "source": [
    "The Bert implementation comes with a pretrained tokenizer and a definied vocabulary. We load the one related to the smallest pre-trained model bert-base-cased. We use the cased variate since it is well suited for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "4ac7d09533554d50aeea2f9acaf1a2cf",
      "ff9b111b76f94bbea75b1349ab8ec52a",
      "2daa795f9f8d449c9c2bafc0e73dc26f",
      "5cbcb58d61bf4f66831845a787c8ed4a",
      "098fad80c35647d8a5a55116c201707d",
      "55264d3daa074d55922b65a887146293",
      "295ecb19bdfb434e91e60ea867d10027",
      "22b6d9eb42aa4afe82ddf6ceae2d9922"
     ]
    },
    "executionInfo": {
     "elapsed": 13676,
     "status": "ok",
     "timestamp": 1607959640878,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "JyuwJkHzk37V",
    "outputId": "1ff1d6ca-d624-43fa-a3c4-44158bdc7059",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 12438,
     "status": "ok",
     "timestamp": 1607959640879,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "wncvf0cIk37V"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return ['[CLS]'] + tokenized_sentence + ['[SEP]'], ['O'] + labels + ['O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 26050,
     "status": "ok",
     "timestamp": 1607959656728,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "fwJjDbRhk37V"
   },
   "outputs": [],
   "source": [
    "train_tokenized = [tokenize_and_preserve_labels(s, l, tokenizer) for s, l in zip(train_data, train_labels)]\n",
    "valid_tokenized = [tokenize_and_preserve_labels(s, l, tokenizer) for s, l in zip(valid_data, valid_labels)]\n",
    "test_tokenized = [tokenize_and_preserve_labels(s, l, tokenizer) for s, l in zip(test_data, test_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 24983,
     "status": "ok",
     "timestamp": 1607959656730,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "BX0XNssQk37V"
   },
   "outputs": [],
   "source": [
    "train_tokens = [x[0] for x in train_tokenized]\n",
    "train_labels = [x[1] for x in train_tokenized]\n",
    "valid_tokens = [x[0] for x in valid_tokenized]\n",
    "valid_labels = [x[1] for x in valid_tokenized]\n",
    "test_tokens = [x[0] for x in test_tokenized]\n",
    "test_labels = [x[1] for x in test_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 23672,
     "status": "ok",
     "timestamp": 1607959656730,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "K58-KFW3k37V"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = np.max([len(sen) for sen in train_tokens])\n",
    "        \n",
    "MEAN_SEQ_LENGTH = int(np.mean([len(sen) for sen in train_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "print(MAX_SEQ_LENGTH)\n",
    "print(MEAN_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(sen) for sen in train_tokens])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile([len(sen) for sen in train_tokens], 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlzHsMsBk37V"
   },
   "source": [
    "Next, we cut and pad the token and label sequences to our desired length. Let's constaint sequence length with 0.75 quantile - 30 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISIRED_LENGTH = int(np.quantile([len(sen) for sen in train_tokens], 0.75))\n",
    "DISIRED_LENGTH = MAX_SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 1559,
     "status": "ok",
     "timestamp": 1607959661896,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "YdbMzcmqk37V"
   },
   "outputs": [],
   "source": [
    "train_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in train_tokens],\n",
    "                          maxlen=DISIRED_LENGTH, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "train_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in train_labels],\n",
    "                     maxlen=DISIRED_LENGTH, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "valid_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in valid_tokens],\n",
    "                          maxlen=DISIRED_LENGTH, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "valid_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in valid_labels],\n",
    "                     maxlen=DISIRED_LENGTH, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "test_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokens],\n",
    "                          maxlen=DISIRED_LENGTH, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "test_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in test_labels],\n",
    "                     maxlen=DISIRED_LENGTH, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1607959662467,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "_gim76Xck37V",
    "outputId": "6f046483-70bc-49dc-d759-3ec70b9cfd42",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495,\n",
       "       12913,   119,   102,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first train sentence\n",
    "train_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnM-Nw7sk37V"
   },
   "source": [
    "We create the masks to ignore the padded elements in the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 9245,
     "status": "ok",
     "timestamp": 1607959672006,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "AYMFEMztk37V"
   },
   "outputs": [],
   "source": [
    "train_masks = [[float(i != 0.0) for i in ii] for ii in train_ids]\n",
    "valid_masks = [[float(i != 0.0) for i in ii] for ii in valid_ids]\n",
    "test_masks = [[float(i != 0.0) for i in ii] for ii in test_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BjLeYKnk37V"
   },
   "source": [
    "We have to convert the dataset to torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 8166,
     "status": "ok",
     "timestamp": 1607959672008,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "rpI2uOqCk37V",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_ids)\n",
    "train_tags = torch.tensor(train_tags)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "valid_inputs = torch.tensor(valid_ids)\n",
    "valid_tags = torch.tensor(valid_tags)\n",
    "valid_masks = torch.tensor(valid_masks)\n",
    "test_inputs = torch.tensor(test_ids)\n",
    "test_tags = torch.tensor(test_tags)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxBA_eN5k37V"
   },
   "source": [
    "The last step is to define the dataloaders. We shuffle the data at training time with the RandomSampler and at test time we just pass them sequentially with the SequentialSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "executionInfo": {
     "elapsed": 7076,
     "status": "ok",
     "timestamp": 1607959672008,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "6Uiw8EkWk37V"
   },
   "outputs": [],
   "source": [
    "# let's set the batch size\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "executionInfo": {
     "elapsed": 5765,
     "status": "ok",
     "timestamp": 1607959672009,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "oYaKfiMpk37V"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_masks, train_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_data = TensorDataset(valid_inputs, valid_masks, valid_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtlwOjQkk37V"
   },
   "source": [
    "### BERT setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OFDSv-Dk37V"
   },
   "source": [
    "The transformer package provides a BertForTokenClassification class for token-level predictions. BertForTokenClassification is a fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7252,
     "status": "ok",
     "timestamp": 1607959675516,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "J9gDyERuSECv",
    "outputId": "bc94d967-6151-4704-fba5-6f344c4fb068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-crf\n",
      "  Downloading https://files.pythonhosted.org/packages/96/7d/4c4688e26ea015fc118a0327e5726e6596836abce9182d3738be8ec2e32a/pytorch_crf-0.7.2-py3-none-any.whl\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 5778,
     "status": "ok",
     "timestamp": 1607959675517,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "o_r0zwsqk37W"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, AdamW\n",
    "from torch import nn\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "executionInfo": {
     "elapsed": 5174,
     "status": "ok",
     "timestamp": 1607959675517,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "Jbp3ZMkMrJv5"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1607959677688,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "OS_lcETVk37W"
   },
   "outputs": [],
   "source": [
    "# Bert pre-trained model selected in the list: bert-base-uncased, \n",
    "# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n",
    "# bert-base-multilingual-cased, bert-base-chinese.\n",
    "BERT_MODEL = 'bert-base-cased'\n",
    "\n",
    "# The name of the task to train.I'm going to name this 'yelp'.\n",
    "TASK_NAME = 'first'\n",
    "\n",
    "# The output directory where the fine-tuned model and checkpoints will be written.\n",
    "OUTPUT_DIR = f'outputs/{TASK_NAME}/'\n",
    "\n",
    "# The directory where the evaluation reports will be written to.\n",
    "REPORTS_DIR = f'reports/{TASK_NAME}_evaluation_report/'\n",
    "\n",
    "\n",
    "# This is where BERT will look for pre-trained models to load parameters from.\n",
    "CACHE_DIR = 'cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "executionInfo": {
     "elapsed": 2018,
     "status": "ok",
     "timestamp": 1607959681106,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "IPgCix_zk37W"
   },
   "outputs": [],
   "source": [
    "class Our_model(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT+biLSTM+Linear+Relu\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=128, num_labels=len(tag2idx), bert_layers=1, concat=True):\n",
    "        \"\"\"\n",
    "        Creates model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size:\n",
    "        hidden_size:\n",
    "        num_labels:\n",
    "        bert_layers: int, default=1\n",
    "            Num of final BERT hidden layers to be used as embedding vector.\n",
    "        concat: bool, default=True\n",
    "            Whether to concat (True) or sum (False) last BERT hidden layers.\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Our_model, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.bert_layers = bert_layers\n",
    "        self.concat = concat\n",
    "        \n",
    "        self.bert = BertForTokenClassification.from_pretrained(\n",
    "                        BERT_MODEL, #cache_dir=CACHE_DIR,\n",
    "                        output_hidden_states=True)\n",
    "        \n",
    "        for pars in self.bert.parameters():\n",
    "            pars.requires_grad = False\n",
    "        \n",
    "        embedding_dim = self.bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        if self.concat:\n",
    "            self.bilstm = nn.LSTM(embedding_dim*self.bert_layers, self.hidden_size,\n",
    "                                  bidirectional=True)\n",
    "        else:\n",
    "            self.bilstm = nn.LSTM(embedding_dim, self.hidden_size,\n",
    "                                  bidirectional=True)\n",
    "        \n",
    "        self.linear = nn.Linear(self.hidden_size*2, self.num_labels)\n",
    "        self.crf = CRF(num_tags=self.num_labels, batch_first=True)\n",
    "    \n",
    "    def forward(self, sequence, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward propogate of model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence:\n",
    "        attention_mask:\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Logits\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        bert_hiddens = self.bert(sequence, attention_mask=attention_mask)[1]\n",
    "        if self.concat:\n",
    "            bert_embedding = torch.cat(bert_hiddens[-self.bert_layers:], dim=2)#[bert_hiddens[-i] for i in range(-1, -self.bert_layers-1, -1)], dim=0)\n",
    "        else:\n",
    "            emb_sum = 0\n",
    "            for h in bert_hiddens[-self.bert_layers:]:\n",
    "                emb_sum += h\n",
    "            bert_embedding = emb_sum\n",
    "    \n",
    "        bilstm_output, (h_n, c_n) = self.bilstm(bert_embedding)\n",
    "        linear_output = nn.functional.relu(self.linear(bilstm_output))\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRdo55TsCW01"
   },
   "source": [
    "If we need scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 761,
     "status": "ok",
     "timestamp": 1607854760807,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "WBGrwIomk37W"
   },
   "outputs": [],
   "source": [
    "#from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrUTY1c8k37W"
   },
   "outputs": [],
   "source": [
    "#total_steps = len(train_dataloader) * NUM_TRAIN_EPOCHS\n",
    "\n",
    "#scheduler = get_linear_schedule_with_warmup(\n",
    "#    optimizer,\n",
    "#    num_warmup_steps=0,\n",
    "#    num_training_steps=total_steps\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgaoD-uyk37W"
   },
   "source": [
    "Let's use seqeval library for sequence labeling evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8610,
     "status": "ok",
     "timestamp": 1607959692664,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "dqMh3xEcSVyh",
    "outputId": "26ccde88-cca3-4691-eee2-6df6222d2a15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
      "\r",
      "\u001b[K     |███████▌                        | 10kB 14.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 20kB 18.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 30kB 17.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 40kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 3.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=4e941855eef92f8adb3154eb72a82e7f3b2d85bcb40a2854ed9c454a7bba0b8e\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 7588,
     "status": "ok",
     "timestamp": 1607959692665,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "QRu2-WGTk37W"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1607959993512,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "GqlaHagIk37W",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, scheduler=None, n_epoch=5,\n",
    "          max_grad_norm=None, validate=True, valid_dataloader=None,\n",
    "          show_info=True):\n",
    "    loss_values = []\n",
    "    if validate and valid_dataloader is not None:\n",
    "        validation_loss_values = []\n",
    "        valid_accuracies = []\n",
    "        valid_f1_scores = []\n",
    "\n",
    "    for _ in range(n_epoch):\n",
    "    \n",
    "        # Training\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        if show_info:\n",
    "            enumerator = enumerate(tqdm.tqdm(train_dataloader, position=0, leave=True))\n",
    "        else:\n",
    "            enumerator = enumerate(train_dataloader)\n",
    "\n",
    "        for step, batch in enumerator:\n",
    "            if device.type != 'cpu':\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            model.zero_grad()\n",
    "\n",
    "            logits = model.forward(b_input_ids, b_input_mask.byte())\n",
    "            \n",
    "            # because we need negative log likelyhood\n",
    "            loss = -1*model.crf.forward(logits, b_labels, mask=b_input_mask.byte())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if show_info:\n",
    "                if step % 4 == 0 and step > 0:\n",
    "                    print(f\"\\n{step}: avg loss per batch: {total_loss/step}\\n\")\n",
    "\n",
    "            if max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(parameters=model.parameters(),\n",
    "                                            max_norm=max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        if show_info:\n",
    "            print(f\"Average train loss: {avg_train_loss}\")\n",
    "\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        if validate and valid_dataloader is not None:\n",
    "          # Validation\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            eval_loss, eval_accuracy = 0, 0\n",
    "            predictions, true_labels = [], []\n",
    "\n",
    "            for batch in valid_dataloader:\n",
    "                if device.type != 'cpu':\n",
    "                    batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = model.forward(b_input_ids, b_input_mask.byte())\n",
    "                    loss = -1*model.crf.forward(logits, b_labels, mask=b_input_mask.byte())\n",
    "                    tags = model.crf.decode(logits, mask=b_input_mask.byte())\n",
    "\n",
    "                # move loss to cpu\n",
    "                eval_loss += loss.item()\n",
    "                predictions.extend(tags)\n",
    "                labels_ = b_labels.detach().cpu().numpy()\n",
    "                true_labels.extend(labels_)\n",
    "\n",
    "            eval_loss = eval_loss / len(valid_dataloader)\n",
    "            validation_loss_values.append(eval_loss)\n",
    "            if show_info:\n",
    "                print(f\"Validation loss: {eval_loss}\")\n",
    "\n",
    "            all_predicted_tags = []\n",
    "            for s in predictions:\n",
    "                tag_names = [idx2tag[i] for i in s]\n",
    "                all_predicted_tags.append(tag_names)\n",
    "\n",
    "            all_true_tags = []\n",
    "            for s in true_labels:\n",
    "                tag_names = [idx2tag[i] for i in s if idx2tag[i] != 'PAD']\n",
    "                all_true_tags.append(tag_names)\n",
    "\n",
    "            valid_acc = accuracy_score(all_predicted_tags, all_true_tags)\n",
    "            valid_f1 = f1_score(all_predicted_tags, all_true_tags)\n",
    "            valid_accuracies.append(valid_acc)\n",
    "            valid_f1_scores.append(valid_f1)\n",
    "\n",
    "            if show_info:\n",
    "                print(f\"Validation accuracy: {valid_acc}\")\n",
    "                print(f\"Validation F1-score: {valid_f1}\\n\")\n",
    "\n",
    "    return loss_values, validation_loss_values, valid_accuracies, valid_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1607959698825,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "Bn1SZDqG9NTK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ByjTJrC8P2K"
   },
   "source": [
    "Fix some train parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 1014,
     "status": "ok",
     "timestamp": 1607959701171,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "AnXnYO3H8GDs"
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPOCHS = 2\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "N_FOLDS = 3\n",
    "\n",
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 1061,
     "status": "ok",
     "timestamp": 1607959704147,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "Rl13L8v75q0t"
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_7d6NpZOZ4w"
   },
   "outputs": [],
   "source": [
    "# model = Our_model(batch_size=BATCH_SIZE, hidden_size=128, num_labels=len(tag2idx),\n",
    "#                     bert_layers=2, concat=False)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJR2I5SCOi_E"
   },
   "outputs": [],
   "source": [
    "# opt = AdamW(params=model.parameters(),lr=1e-3)\n",
    "# train(model, train_dataloader, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parameteres search let's leave just 1/5 of training examples (~2800)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_indexes = np.random.randint(0, len(train_data), int(len(train_data)/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = torch.utils.data.Subset(train_data, small_train_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "907654c2ffc247af8ed6a878399275b9",
      "c7de314c9ca943e89a4fa4f1c3fce657",
      "10a7845fcae44dcfa14b588cc06736ed",
      "c2663db37a9549ab9bbda653f1b73981",
      "4cf0ed4903bc40ca8d7595299b57a5b7",
      "b9f88b3a0ec54f64906bdb941f820299",
      "6847cff955fd477799d21cbf765de1c1",
      "a8109c411a29479ba560f8ae5e530db5",
      "3bb3fa5edb5b4aeab35df46b1830175e",
      "e44f64d5b830469b992eb70e224af96d",
      "690bbb1cd73a4b0c87047af83ce3dc35",
      "a04e325d469e446ea442f6a9d99e2d23",
      "8c3c55c623d749fe88cc57b4dff94617",
      "f04ff6a9e2c1428da5bd324b8dd9cda6",
      "d7cd4ea5df4c449eab12556c6abe3058",
      "000f971e6c534e2d882572d5562de721"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1079734,
     "status": "ok",
     "timestamp": 1607961112063,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "BYCHQ6sboU8R",
    "outputId": "08a0c29d-5e2d-4ba2-bdcc-84469ae10509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 5.72 µs\n",
      "Model #0 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/danilculkov/opt/anaconda3/envs/diploma/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PAD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #1 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #2 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #3 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #4 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #5 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #6 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #7 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #8 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #9 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #10 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #11 of 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "param_grid = {\n",
    "    'opt': ['AdamW'],\n",
    "    'lr': [3e-4, 7e-4, 1e-3],\n",
    "    'bert_layers': [2,3],\n",
    "    'concat': [True, False],\n",
    "    'max_grad_norm': [None]#[1., None]\n",
    "}\n",
    "\n",
    "#param_grid = {\n",
    "#    'opt': ['AdamW'],\n",
    "#    'lr': [1e-3],\n",
    "#    'bert_layers': [2],\n",
    "#    'concat': [False],\n",
    "#    'max_grad_norm': [None]\n",
    "#}\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "params_results = {}\n",
    "\n",
    "for m, ps in enumerate(grid):\n",
    "    print(f\"Model #{m} of {len(grid)}\")\n",
    "    _p_r = {'params': ps}\n",
    "\n",
    "    mean_train_losses = 0\n",
    "    mean_valid_losses = 0\n",
    "    mean_valid_accs = 0\n",
    "    mean_valid_f1s = 0\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(small_train)):\n",
    "        train_fold = torch.utils.data.Subset(train_data, train_index)\n",
    "        valid_fold = torch.utils.data.Subset(train_data, valid_index)\n",
    "        _train_dataloader = DataLoader(train_fold, batch_size=BATCH_SIZE)\n",
    "        _valid_dataloader = DataLoader(valid_fold, batch_size=BATCH_SIZE)\n",
    "\n",
    "        model = Our_model(hidden_size=128, \n",
    "                          bert_layers=ps['bert_layers'],\n",
    "                          concat=ps['concat'])\n",
    "        if device.type != 'cpu':\n",
    "            model.to(device)\n",
    "\n",
    "        if ps['opt'] == 'Adam':\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(),lr=ps['lr'])\n",
    "        else:\n",
    "            optimizer = AdamW(params=model.parameters(),lr=ps['lr'])\n",
    "\n",
    "        train_losses, valid_losses, valid_accs, valid_f1s = train(model,\n",
    "                                                _train_dataloader,\n",
    "                                                optimizer,\n",
    "                                                n_epoch=N_EPOCHS,\n",
    "                                                max_grad_norm=ps['max_grad_norm'],\n",
    "                                                valid_dataloader=_valid_dataloader,\n",
    "                                                show_info=False)\n",
    "    \n",
    "        mean_train_losses += np.array(train_losses)\n",
    "        mean_valid_losses += np.array(valid_losses)\n",
    "        mean_valid_accs += np.array(valid_accs)\n",
    "        mean_valid_f1s += np.array(valid_f1s)\n",
    "  \n",
    "    mean_train_losses /= N_FOLDS\n",
    "    mean_valid_losses /= N_FOLDS\n",
    "    mean_valid_accs /= N_FOLDS\n",
    "    mean_valid_f1s /= N_FOLDS\n",
    "    _p_r['mean_train_losses'] = list(mean_train_losses)\n",
    "    _p_r['mean_valid_losses'] = list(mean_valid_losses)\n",
    "    _p_r['mean_valid_accs'] = list(mean_valid_accs)\n",
    "    _p_r['mean_valid_f1s'] = list(mean_valid_f1s)\n",
    "    params_results[m] = _p_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1607959782539,
     "user": {
      "displayName": "Данил Чулков",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXg59tvizvMhVkFE0AGiHp97NpSUJ0FP6HxMfTMw=s64",
      "userId": "01341217299824014552"
     },
     "user_tz": -180
    },
    "id": "Svi7ryb7mKYE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'params': {'bert_layers': 2,\n",
       "   'concat': True,\n",
       "   'lr': 0.0003,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [46461.16015625],\n",
       "  'mean_valid_losses': [42868.494791666664],\n",
       "  'mean_valid_accs': [0.4307256358067166],\n",
       "  'mean_valid_f1s': [0.04306148100847803]},\n",
       " 1: {'params': {'bert_layers': 2,\n",
       "   'concat': True,\n",
       "   'lr': 0.0007,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [44169.748046875],\n",
       "  'mean_valid_losses': [35965.94140625],\n",
       "  'mean_valid_accs': [0.7669562096182889],\n",
       "  'mean_valid_f1s': [0.00021729682746631898]},\n",
       " 2: {'params': {'bert_layers': 2,\n",
       "   'concat': True,\n",
       "   'lr': 0.001,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [41096.2890625],\n",
       "  'mean_valid_losses': [28607.479166666668],\n",
       "  'mean_valid_accs': [0.7670739646750677],\n",
       "  'mean_valid_f1s': [0.0]},\n",
       " 3: {'params': {'bert_layers': 2,\n",
       "   'concat': False,\n",
       "   'lr': 0.0003,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [46515.856119791664],\n",
       "  'mean_valid_losses': [44757.25390625],\n",
       "  'mean_valid_accs': [0.20571519105051403],\n",
       "  'mean_valid_f1s': [0.039091303465210135]},\n",
       " 4: {'params': {'bert_layers': 2,\n",
       "   'concat': False,\n",
       "   'lr': 0.0007,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [46264.664713541664],\n",
       "  'mean_valid_losses': [39332.971354166664],\n",
       "  'mean_valid_accs': [0.7251095093512681],\n",
       "  'mean_valid_f1s': [0.02005720772293909]},\n",
       " 5: {'params': {'bert_layers': 2,\n",
       "   'concat': False,\n",
       "   'lr': 0.001,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [44660.830729166664],\n",
       "  'mean_valid_losses': [34984.118489583336],\n",
       "  'mean_valid_accs': [0.7662094961922269],\n",
       "  'mean_valid_f1s': [0.0012300972657078512]},\n",
       " 6: {'params': {'bert_layers': 3,\n",
       "   'concat': True,\n",
       "   'lr': 0.0003,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [45452.657552083336],\n",
       "  'mean_valid_losses': [40443.790364583336],\n",
       "  'mean_valid_accs': [0.660743739285646],\n",
       "  'mean_valid_f1s': [0.03895673985289561]},\n",
       " 7: {'params': {'bert_layers': 3,\n",
       "   'concat': True,\n",
       "   'lr': 0.0007,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [42963.788411458336],\n",
       "  'mean_valid_losses': [32864.886067708336],\n",
       "  'mean_valid_accs': [0.7525285101296132],\n",
       "  'mean_valid_f1s': [0.010787486515641856]},\n",
       " 8: {'params': {'bert_layers': 3,\n",
       "   'concat': True,\n",
       "   'lr': 0.001,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [42898.564453125],\n",
       "  'mean_valid_losses': [30491.704427083332],\n",
       "  'mean_valid_accs': [0.767057803058906],\n",
       "  'mean_valid_f1s': [0.0]},\n",
       " 9: {'params': {'bert_layers': 3,\n",
       "   'concat': False,\n",
       "   'lr': 0.0003,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [44457.08203125],\n",
       "  'mean_valid_losses': [41441.3671875],\n",
       "  'mean_valid_accs': [0.48886944415766687],\n",
       "  'mean_valid_f1s': [0.027975239955695164]},\n",
       " 10: {'params': {'bert_layers': 3,\n",
       "   'concat': False,\n",
       "   'lr': 0.0007,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [45987.960286458336],\n",
       "  'mean_valid_losses': [40501.256510416664],\n",
       "  'mean_valid_accs': [0.6090687202411621],\n",
       "  'mean_valid_f1s': [0.028702752319777692]},\n",
       " 11: {'params': {'bert_layers': 3,\n",
       "   'concat': False,\n",
       "   'lr': 0.001,\n",
       "   'max_grad_norm': None,\n",
       "   'opt': 'AdamW'},\n",
       "  'mean_train_losses': [43712.453125],\n",
       "  'mean_valid_losses': [35211.927734375],\n",
       "  'mean_valid_accs': [0.7451119632426507],\n",
       "  'mean_valid_f1s': [0.015398905132952551]}}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After train on 1 epoch on small dataset the best result gave the model that concatenates two last bert layers and has learning rate 3e-4, so let's train such model on all train data. Also, let's increase LSTM hidden size to be 512 and use now linear scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) *  N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Our_model(hidden_size=512, bert_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 5/28 [09:06<41:45, 108.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4: avg loss per batch: 6314.7862548828125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 9/28 [1:26:33<5:09:41, 978.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8: avg loss per batch: 5550.369323730469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 13/28 [4:02:17<7:44:43, 1858.92s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12: avg loss per batch: 5276.091715494792\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 17/28 [4:11:14<1:40:53, 550.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16: avg loss per batch: 5036.2574462890625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 21/28 [4:20:11<27:23, 234.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20: avg loss per batch: 4873.363818359375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 25/28 [4:28:59<07:42, 154.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24: avg loss per batch: 4745.4034423828125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [4:34:04<00:00, 587.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 4433.7581133161275\n",
      "Validation loss: 3857.615077427455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8888485923060391\n",
      "Validation F1-score: 0.507137874912811\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 5/28 [09:22<42:58, 112.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4: avg loss per batch: 5129.735290527344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 9/28 [1:50:44<6:50:35, 1296.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8: avg loss per batch: 4646.592498779297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 13/28 [1:59:03<1:41:29, 405.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12: avg loss per batch: 4489.362711588542\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 17/28 [2:06:48<33:56, 185.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16: avg loss per batch: 4343.702392578125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 21/28 [2:14:52<15:54, 136.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20: avg loss per batch: 4267.050317382813\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 25/28 [2:23:14<06:21, 127.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24: avg loss per batch: 4202.575876871745\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [2:27:57<00:00, 317.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 3949.8855503627233\n",
      "Validation loss: 3600.4528459821427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8914275736084246\n",
      "Validation F1-score: 0.5186604617209598\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 5/28 [09:32<44:26, 115.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4: avg loss per batch: 4816.748107910156\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 9/28 [17:06<36:11, 114.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8: avg loss per batch: 4345.922790527344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 13/28 [24:33<28:04, 112.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12: avg loss per batch: 4143.819315592448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 17/28 [32:02<20:34, 112.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16: avg loss per batch: 4070.201934814453\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 21/28 [39:40<13:20, 114.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20: avg loss per batch: 4045.1505126953125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 25/28 [47:18<05:43, 114.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24: avg loss per batch: 4002.333913167318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [51:56<00:00, 111.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 3772.6775643484934\n",
      "Validation loss: 3480.8030308314733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8931871910595315\n",
      "Validation F1-score: 0.5319676648906854\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 5/28 [09:35<43:55, 114.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4: avg loss per batch: 4915.473571777344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 9/28 [17:09<36:01, 113.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8: avg loss per batch: 4362.029083251953\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 13/28 [47:20<1:19:22, 317.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12: avg loss per batch: 4146.996419270833\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 17/28 [59:14<39:40, 216.40s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16: avg loss per batch: 4034.5235900878906\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 21/28 [1:10:21<19:52, 170.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20: avg loss per batch: 4014.0308837890625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 25/28 [1:19:07<07:02, 140.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24: avg loss per batch: 3929.1710001627603\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [1:24:22<00:00, 180.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 3680.3513793945312\n",
      "Validation loss: 3432.055890764509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8935767246937459\n",
      "Validation F1-score: 0.5329735662672641\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 5/28 [09:45<45:19, 118.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4: avg loss per batch: 4587.220031738281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 9/28 [18:08<39:46, 125.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8: avg loss per batch: 4165.866729736328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 13/28 [26:39<32:04, 128.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12: avg loss per batch: 3970.5042928059897\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 17/28 [35:13<23:38, 128.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16: avg loss per batch: 3935.6510314941406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 21/28 [43:31<14:45, 126.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20: avg loss per batch: 3891.0838989257813\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 25/28 [51:22<05:56, 118.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24: avg loss per batch: 3866.463165283203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [56:08<00:00, 120.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 3637.530604771205\n",
      "Validation loss: 3410.8293805803573\n",
      "Validation accuracy: 0.8939528261336772\n",
      "Validation F1-score: 0.5350283519297604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(params=model.parameters(),lr=3e-4)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "if device.type != 'cpu':\n",
    "    model.to(device)\n",
    "\n",
    "#train_losses, valid_losses, valid_accs, valid_f1s\n",
    "results = train(model, train_dataloader, optimizer, scheduler, n_epoch=N_EPOCHS,\n",
    "     validate=True, valid_dataloader=valid_dataloader)\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "ufCd6_qak37W"
   },
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    # add batch to gpu\n",
    "    #batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Always clear any previously calculated gradients before performing a backward pass.\n",
    "    # forward pass\n",
    "    # This will return the loss (rather than the model output)\n",
    "    # because we have provided the `labels`.\n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(b_input_ids, b_labels)\n",
    "        loss = model.crf.forward(logits, b_labels, b_input_mask.byte())\n",
    "        tags = model.crf.decode(logits, b_input_mask.byte())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXHvKqAkk37W"
   },
   "outputs": [],
   "source": [
    "pred = [[idx2tag[i] for i in tags[0]], [idx2tag[i] for i in tags[1]]]\n",
    "true = [[idx2tag[l.item()] for l in true_labels[0] if tag2idx['PAD'] != l], [idx2tag[l.item()] for l in true_labels[1] if tag2idx['PAD'] != l]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ex = [['O', 'O', 'O', 'O', 'O', 'O'], ['LOC', 'O', 'O', 'LOC', 'O', 'O', 'PER']]\n",
    "pred_ex = [['O', 'O', 'O', 'O', 'O', 'O'], ['LOC', 'O', 'O', 'O', 'O', 'O', 'O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461538461538461"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(true_ex, pred_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_acc_score(true_labels, pred_labels):\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for true_seq, pred_seq in zip(true_labels, pred_labels):\n",
    "        all_true.extend(true_seq)\n",
    "        all_pred.extend(pred_seq)\n",
    "    N = len(all_true)\n",
    "    ok = np.sum(np.asarray(all_true) == np.asarray(all_pred))\n",
    "    return ok/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461538461538461"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_acc_score(true_ex, pred_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(true_ex, pred_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_f1_score(true_labels, pred_labels, eval_way='strict'):\n",
    "    \"\"\"\n",
    "    F1-score measure for multiple heads case.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    true_labels:\n",
    "    pred_labels:\n",
    "    eval_way: str, default='strict'\n",
    "        Way to measeare f1-score. Could be 'strict', 'exact', 'partial' or 'type'.\n",
    "        See: http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
    "\n",
    "    \"\"\"\n",
    "    all_correct, all_incorrect = 0, 0\n",
    "    all_missed, all_spurious = 0, 0, 0\n",
    "    if eval_way == 'partial':\n",
    "        all_partial = 0\n",
    "    for true_seq, pred_seq in zip(true_labels, pred_labels):\n",
    "        correct_tags, incorrect_tags = 0, 0\n",
    "        missed_tags, spurious_tags, fault_tags = 0, 0, 0\n",
    "        \n",
    "        for t, p in zip(true_seq, pred_seq):\n",
    "            if t == p:\n",
    "                correct_tags += 1\n",
    "            else:\n",
    "                incorrect_tags += 1\n",
    "                if t == 'O':\n",
    "                    spurious_tags += 1\n",
    "                else:\n",
    "                    if p == 'O':\n",
    "                        missed_tags += 1\n",
    "                    else:\n",
    "                        fault_tags += 1\n",
    "        \n",
    "        if incorrect_tags == 0:\n",
    "            all_correct += 1\n",
    "            continue\n",
    "        \n",
    "        if eval_way == 'strict':\n",
    "            if missed_tags > 0 and spurious_tags == 0 and fault_tags == 0:\n",
    "                all_missed += 1\n",
    "            elif missed_tags == 0 and spurious_tags > 0 and fault_tags == 0:\n",
    "                all_spurious += 1\n",
    "            else:\n",
    "                all_incorrect += 1\n",
    "        elif eval_way == 'exact':\n",
    "            if missed_tags == 0 and spurious_tags == 0:\n",
    "                all_correct += 1 # the same boundaries\n",
    "            elif missed_tags > 0 and spurious_tags == 0 and fault_tags == 0:\n",
    "                all_missed += 1\n",
    "            elif missed_tags == 0 and spurious_tags > 0 and fault_tags == 0:\n",
    "                all_spurious += 1\n",
    "            else:\n",
    "                all_incorrect += 1\n",
    "        elif eval_way == 'partial':\n",
    "            if fault_tags > 0:\n",
    "                all_partial += 1\n",
    "            elif missed_tags > 0 and spurious_tags == 0:\n",
    "                all_missed += 1\n",
    "            elif missed_tags == 0 and spurious_tags > 0:\n",
    "                all_spurious += 1\n",
    "            else:\n",
    "                all_incorrect += 1\n",
    "        elif eval_way == 'type':\n",
    "            if correct_tags > 0:\n",
    "                all_correct += 1\n",
    "            elif missed_tags > 0 and spurious_tags == 0 and fault_tags == 0:\n",
    "                all_missed += 1\n",
    "            elif missed_tags == 0 and spurious_tags > 0 and fault_tags == 0:\n",
    "                all_spurious += 1\n",
    "            else:\n",
    "                all_incorrect += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "diploma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "000f971e6c534e2d882572d5562de721": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "098fad80c35647d8a5a55116c201707d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "10a7845fcae44dcfa14b588cc06736ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9f88b3a0ec54f64906bdb941f820299",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4cf0ed4903bc40ca8d7595299b57a5b7",
      "value": 433
     }
    },
    "22b6d9eb42aa4afe82ddf6ceae2d9922": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "295ecb19bdfb434e91e60ea867d10027": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2daa795f9f8d449c9c2bafc0e73dc26f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55264d3daa074d55922b65a887146293",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_098fad80c35647d8a5a55116c201707d",
      "value": 213450
     }
    },
    "3bb3fa5edb5b4aeab35df46b1830175e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_690bbb1cd73a4b0c87047af83ce3dc35",
       "IPY_MODEL_a04e325d469e446ea442f6a9d99e2d23"
      ],
      "layout": "IPY_MODEL_e44f64d5b830469b992eb70e224af96d"
     }
    },
    "4ac7d09533554d50aeea2f9acaf1a2cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2daa795f9f8d449c9c2bafc0e73dc26f",
       "IPY_MODEL_5cbcb58d61bf4f66831845a787c8ed4a"
      ],
      "layout": "IPY_MODEL_ff9b111b76f94bbea75b1349ab8ec52a"
     }
    },
    "4cf0ed4903bc40ca8d7595299b57a5b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "55264d3daa074d55922b65a887146293": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cbcb58d61bf4f66831845a787c8ed4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22b6d9eb42aa4afe82ddf6ceae2d9922",
      "placeholder": "​",
      "style": "IPY_MODEL_295ecb19bdfb434e91e60ea867d10027",
      "value": " 213k/213k [00:00&lt;00:00, 571kB/s]"
     }
    },
    "6847cff955fd477799d21cbf765de1c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "690bbb1cd73a4b0c87047af83ce3dc35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f04ff6a9e2c1428da5bd324b8dd9cda6",
      "max": 435779157,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c3c55c623d749fe88cc57b4dff94617",
      "value": 435779157
     }
    },
    "8c3c55c623d749fe88cc57b4dff94617": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "907654c2ffc247af8ed6a878399275b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_10a7845fcae44dcfa14b588cc06736ed",
       "IPY_MODEL_c2663db37a9549ab9bbda653f1b73981"
      ],
      "layout": "IPY_MODEL_c7de314c9ca943e89a4fa4f1c3fce657"
     }
    },
    "a04e325d469e446ea442f6a9d99e2d23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_000f971e6c534e2d882572d5562de721",
      "placeholder": "​",
      "style": "IPY_MODEL_d7cd4ea5df4c449eab12556c6abe3058",
      "value": " 436M/436M [00:25&lt;00:00, 17.2MB/s]"
     }
    },
    "a8109c411a29479ba560f8ae5e530db5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9f88b3a0ec54f64906bdb941f820299": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2663db37a9549ab9bbda653f1b73981": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8109c411a29479ba560f8ae5e530db5",
      "placeholder": "​",
      "style": "IPY_MODEL_6847cff955fd477799d21cbf765de1c1",
      "value": " 433/433 [00:00&lt;00:00, 1.06kB/s]"
     }
    },
    "c7de314c9ca943e89a4fa4f1c3fce657": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7cd4ea5df4c449eab12556c6abe3058": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e44f64d5b830469b992eb70e224af96d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f04ff6a9e2c1428da5bd324b8dd9cda6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff9b111b76f94bbea75b1349ab8ec52a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
