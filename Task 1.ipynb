{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    with open(path, 'r') as f:\n",
    "        text_data = f.read()\n",
    "    for l in text_data.split('\\n'):\n",
    "        yield l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDF(path):\n",
    "    dicts = []\n",
    "    for d in parse(path):\n",
    "        #print(d)\n",
    "        try:\n",
    "            dicts.append(json.loads(d))\n",
    "        except:\n",
    "            print(d)\n",
    "            print(type(d))\n",
    "    return pd.DataFrame(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "data = getDF('Data/Movies_and_TV_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123960"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reviewerID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['isGood'] = data['overall'] > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',\n",
       "       'overall', 'summary', 'unixReviewTime', 'reviewTime', 'isGood'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первые шаги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем обучать модель с помощью стохастического градиента. BFGS на данных упорно не хочет сходиться даже при увеличении верхней границы числа итераций, в то время как SAG как раз рекомендован в документации для разреженных данных и данных большого размера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_data, test_data, train_ans, test_ans] = train_test_split(data['reviewText'], data['isGood'],train_size=0.8, test_size=0.2, shuffle=True, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train = vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x712164 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 184 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_first = LogisticRegression(solver='sag', random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=25, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_first.fit(transformed_train, train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196446836742547"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_ans, clf_first.predict(transformed_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ого!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем лучший параметр регуляризации с помощью кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfCv = LogisticRegressionCV(Cs=[1.0,1.2,1.4,1.6], cv=5, scoring='f1', random_state=25, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=[1.0, 1.2, 1.4, 1.6], class_weight=None, cv=5,\n",
       "                     dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
       "                     l1_ratios=None, max_iter=100, multi_class='auto',\n",
       "                     n_jobs=None, penalty='l2', random_state=25, refit=True,\n",
       "                     scoring='f1', solver='sag', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfCv.fit(transformed_train, train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfCv.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, классификатор достигает наибольшего успеха при параметре регуляризации C, равном 1.2, будем использовать его и далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196452546328897"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_ans, clfCv.predict(transformed_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало не сильно лучше, но в любом случае, мы получили какой-то просто потрясающий результат. Посмотрим, что будет, если поиграться с параметрами tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-грамы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### биграм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_2 = TfidfVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_2 = vectorizer_2.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1.2, random_state=25, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=25, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(transformed_train_2, train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_2 = vectorizer_2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9295234169328924"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_ans, clf.predict(transformed_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшой, но прогресс!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-грам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer_3 = TfidfVectorizer(ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_3 = vectorizer_3.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_3 = LogisticRegression(C=1.2, random_state=25, solver='sag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то kernel прилег((("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1,2) - грам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_12 = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_12 = vectorizer_12.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_12 = LogisticRegression(C=1.2, random_state=25, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=25, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_12.fit(transformed_train_12, train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_12 = vectorizer_12.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9325761802089773"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_ans, clf_12.predict(transformed_test_12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще круче!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Редкие и частые слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_df = 0.9\n",
      "0.9196452546328897\n",
      "max_df = 0.92\n",
      "0.9196452546328897\n",
      "max_df = 0.9400000000000001\n",
      "0.9196452546328897\n",
      "max_df = 0.9600000000000001\n",
      "0.9196452546328897\n",
      "max_df = 0.9800000000000001\n",
      "0.9196452546328897\n",
      "max_df = 1.0\n",
      "0.9196452546328897\n"
     ]
    }
   ],
   "source": [
    "for max_df in np.arange(0.9, 1.01, 0.02):\n",
    "    print(f\"max_df = {max_df}\")\n",
    "    vectorizer_max_df = TfidfVectorizer(max_df=max_df)\n",
    "    transformed_train_max_df = vectorizer_max_df.fit_transform(train_data)\n",
    "    clf_max_df = LogisticRegression(C=1.2, random_state=25, solver='sag')\n",
    "    clf_max_df.fit(transformed_train_max_df, train_ans)\n",
    "    transformed_test_max_df = vectorizer_max_df.transform(test_data)\n",
    "    print(f1_score(test_ans, clf_max_df.predict(transformed_test_max_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: в данном случае никакие ограничения на максимальную частоту не улучшают качество модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df = 0.0\n",
      "0.9196452546328897\n",
      "min_df = 0.01\n",
      "0.9050043721369029\n",
      "min_df = 0.02\n",
      "0.8959838277432098\n",
      "min_df = 0.03\n",
      "0.8875914291504688\n",
      "min_df = 0.04\n",
      "0.8852227163106108\n",
      "min_df = 0.05\n",
      "0.8809551645525224\n"
     ]
    }
   ],
   "source": [
    "for min_df in np.arange(0.00, 0.06, 0.01):\n",
    "    print(f\"min_df = {min_df}\")\n",
    "    vectorizer_min_df = TfidfVectorizer(min_df=min_df)\n",
    "    transformed_train_min_df = vectorizer_min_df.fit_transform(train_data)\n",
    "    clf_min_df = LogisticRegression(C=1.2, random_state=25, solver='sag')\n",
    "    clf_min_df.fit(transformed_train_min_df, train_ans)\n",
    "    transformed_test_min_df = vectorizer_min_df.transform(test_data)\n",
    "    print(f1_score(test_ans, clf_min_df.predict(transformed_test_min_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df = 1\n",
      "0.9196452546328897\n",
      "min_df = 2\n",
      "0.9196261612057766\n",
      "min_df = 3\n",
      "0.9196449357534976\n",
      "min_df = 4\n",
      "0.9196522274602128\n",
      "min_df = 5\n",
      "0.9196713555083249\n",
      "min_df = 6\n",
      "0.9196620352985355\n",
      "min_df = 7\n",
      "0.9196284071750158\n",
      "min_df = 8\n",
      "0.9196031602980052\n",
      "min_df = 9\n",
      "0.9195919800383388\n"
     ]
    }
   ],
   "source": [
    "for min_df in np.arange(1, 10, 1):\n",
    "    print(f\"min_df = {min_df}\")\n",
    "    vectorizer_min_df = TfidfVectorizer(min_df=min_df)\n",
    "    transformed_train_min_df = vectorizer_min_df.fit_transform(train_data)\n",
    "    clf_min_df = LogisticRegression(C=1.2, random_state=25, solver='sag')\n",
    "    clf_min_df.fit(transformed_train_min_df, train_ans)\n",
    "    transformed_test_min_df = vectorizer_min_df.transform(test_data)\n",
    "    print(f1_score(test_ans, clf_min_df.predict(transformed_test_min_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем, что отбрасывание слов, встречающихся менее чем 5 документах дает небольшую прибавку к качеству"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сначала еще немного поковыряться в аргументах tfidf. Кстати, в нем по умолчанию все слова приводятся к нижнему регистру."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем удалить стоп-слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_normalizer_2 = TfidfVectorizer(analyzer='word', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_norm = vectorizer_normalizer_2.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_for_norm = LogisticRegression(C=1.2, random_state=25, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=25, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_for_norm.fit(transformed_train_norm, train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_norm = vectorizer_normalizer_2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9163479905649444"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_ans, clf_for_norm.predict(transformed_test_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление \"стоп-слов\" будто бы даже немного ухудшило качество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну раз так, попробуем сначала вручную преобразовать входные тексты: удалим лишние пробельные символы, выкинем пунктуацию и попробуем выкинуть числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_normalizer(string):\n",
    "    strr = string.lower() #нижний регист\n",
    "    strr = re.sub('\\W', ' ',strr) #выкидываем не буквы и не цифры\n",
    "    strr = re.sub('\\s+', ' ', strr) #заменяем последовательности из пробельных символов на обычные пробелы\n",
    "    strr = strr.strip() #выкидываем пробельные символы в начале и конце текста\n",
    "    return strr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdas 7 65612 vcxvxc s s s s'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_normalizer('SDAS\\n\\n {}-==\\n*7&65612  \\t\\tvcxvxc !!!!,,, s,s,s,s,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "[norm_train_data, norm_test_data, train_ans, test_ans] = train_test_split(data['reviewText'].apply(my_normalizer), data['isGood'],train_size=0.8, test_size=0.2, shuffle=True, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_norm_train = vectorizer.fit_transform(norm_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_for_my_norm = LogisticRegression(C=1.2, random_state=25, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=25, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_for_my_norm.fit(transformed_norm_train, train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_norm_test = vectorizer.transform(norm_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196713555083249"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_ans, clf_for_my_norm.predict(transformed_norm_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это не дает ощутимого прироста к качеству, что довольно как по мне станно. Наверное, это связано с тем, что в tfidf и так по умолчанию встроена определенная предобработка текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выплоним \"удаление акцентов и нормализацию символов на этапе предварительной обработки\" (дословный перевод документации):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_normalizer_1 = TfidfVectorizer(strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transormed_train_norm = vectorizer_normalizer_1.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_for_norm = LogisticRegression(C=1.2, random_state=25, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=25, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_for_norm.fit(transormed_train_norm, train_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test_norm = vectorizer_normalizer_1.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196452546328897"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_ans, clf_for_norm.predict(transformed_test_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никакого профита("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь воспользоваться некоторыми инструментами библиотеки NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут есть два встроенных алгоритма: более универсальный (общий) алгоритм Портера и более агрессивный \"Снежный шарик\". Опробуем первый (так как стемминг всех текстов что-то очень затратным по времени выходит). Воспользуемся также встроенным токенизатором для разбиения текста на слова (токены). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sinister mad scientist, Dr. Goldfoot (Vincent Price), along with his incompetent assistant Igor, is looking to conquer the world by amassing the wealth of all the world'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[578937]['reviewText'][:170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stemmer_func(string):\n",
    "    stemmed_text = ''\n",
    "    for x in word_tokenize(string):\n",
    "        stemmed_text = stemmed_text + (stemmer.stem(x) + ' ')\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sinist mad scientist , dr. goldfoot ( vincent price ) , along with hi incompet assist igor , is look to conquer the world by amass the wealth of all the world'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer_func(data.loc[578937]['reviewText'])[:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162888886389118\n"
     ]
    }
   ],
   "source": [
    "[train_stem_data, test_stem_data, train_ans, test_ans] = train_test_split(data['reviewText'].apply(stemmer_func), data['isGood'],train_size=0.8, test_size=0.2, shuffle=True, random_state=24)\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "transformed_stem_train = vectorizer.fit_transform(train_stem_data)\n",
    "clf_for_stem = LogisticRegression(C=1.2, random_state=25, solver='sag')\n",
    "clf_for_stem.fit(transformed_stem_train, train_ans)\n",
    "transformed_stem_test = vectorizer.transform(test_stem_data)\n",
    "print(f1_score(test_ans, clf_for_stem.predict(transformed_stem_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мда, эта штука обучалась триллион лет и на выходе дала чуть ли не ухудшение качества)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmer_func(string):\n",
    "    lemmed_text = ''\n",
    "    for x in word_tokenize(string):\n",
    "        lemmed_text = lemmed_text + (lemmatizer.lemmatize(x) + ' ')\n",
    "    return lemmed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sinister mad scientist, Dr. Goldfoot (Vincent Price), along with his incompetent assistant Igor, is looking to conquer the world by amassing the wealth of all the world'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[578937]['reviewText'][:170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sinister mad scientist , Dr. Goldfoot ( Vincent Price ) , along with his incompetent assistant Igor , is looking to conquer the world by amassing the wealth of all the world'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer_func(data.loc[578937]['reviewText'])[:175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9185403114287484\n"
     ]
    }
   ],
   "source": [
    "[train_lem_data, test_lem_data, train_ans, test_ans] = train_test_split(data['reviewText'].apply(lemmer_func), data['isGood'],train_size=0.8, test_size=0.2, shuffle=True, random_state=24)\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "transformed_lem_train = vectorizer.fit_transform(train_lem_data)\n",
    "clf_for_lem = LogisticRegression(C=1.2, random_state=25, solver='sag')\n",
    "clf_for_lem.fit(transformed_lem_train, train_ans)\n",
    "transformed_lem_test = vectorizer.transform(test_lem_data)\n",
    "print(f1_score(test_ans, clf_for_lem.predict(transformed_lem_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то она тоже особенно ни к чему не привела."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подведение итогов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поигравшись с различными параметрами tfidf, LogReg, а также попробовав определенные методы предобработки текста, приходим к тому, что лучший результат был достигнут над (1,2)-грамами с выбросом слов, встречающихся менее чем в пяти тренировочных текстах, с параметром регуляризации модели C=1.2 и солвером SAG. Для такого случая score правильно предсказанных ответов тестовой выборки аж более 0.93! Если не очень хочется ждать пока tfidf разберется с (1,2)-грамами, очень даже неплохой результат может быть достигнут также и при обычной юниграмной векторизации с теми же параметрами (полученная оценка гармнонического среднего около 0.92)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
