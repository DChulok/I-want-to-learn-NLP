{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TheBlackCat.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgram on numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "class skip_gram():\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Preprocessing of the text\n",
    "        \"\"\"\n",
    "        text = text.lower() \n",
    "        text = text.strip()\n",
    "        text = sub(\"[^A-Za-z]+\", ' ', text)\n",
    "        text = sub('\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def make_vocab(self, text):\n",
    "        \"\"\"\n",
    "        Returns one-hot-encodded vectors for words from the given text\n",
    "        \"\"\"\n",
    "        text_data = self.preprocess(text)\n",
    "        vocab = dict(Counter(text_data.split()))\n",
    "        V = len(vocab) \n",
    "        one_hot_matrix = np.eye(V, dtype=float)\n",
    "        for i, key in enumerate(sorted(vocab.keys())):\n",
    "            vocab[key] = one_hot_matrix[i][:, np.newaxis]\n",
    "        return vocab\n",
    "    \n",
    "    def __init__(self, text, embedding_size=25):\n",
    "        self.corpus = self.preprocess(text).split()\n",
    "        self.vocab = self.make_vocab(text)\n",
    "        self.V = len(self.vocab)\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.W_in = 0.01*np.random.randn(self.embedding_size, self.V)\n",
    "        self.b_in = np.zeros(self.embedding_size)[:, np.newaxis]\n",
    "        self.W_out = 0.01*np.random.randn(self.V, self.embedding_size)\n",
    "        self.b_out = np.zeros(self.V)[:, np.newaxis]\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes softmax of given tensor\n",
    "        \"\"\"\n",
    "        return np.exp(x)/np.exp(x).sum()\n",
    "    \n",
    "    def predict(self, word):\n",
    "        \"\"\"\n",
    "        Predicts context words by given center word\n",
    "        \"\"\"\n",
    "        projection = self.W_in @ self.vocab[word] + self.b_in\n",
    "        softmax_result = self.softmax(self.W_out @ projection + self.b_out)\n",
    "        return softmax_result\n",
    "    \n",
    "    def get_loss(self, pred, target):\n",
    "        \"\"\"\n",
    "        Returns negative log(P(context_word|center_word))\n",
    "        \"\"\"\n",
    "        #print(target @ pred)\n",
    "        return -np.log((target.T @ pred)/np.sum(pred))\n",
    "        \n",
    "    def fit(self, lr=0.02, window_size=3, epochs=10, return_history=False):\n",
    "        \"\"\"\n",
    "        Fits the model\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch: \", i)\n",
    "            epoch_loss = 0\n",
    "            for i, center_word in enumerate(self.corpus):\n",
    "                #print('center:', center_word)\n",
    "                \n",
    "                context = []\n",
    "                if i < window_size:\n",
    "                    context += self.corpus[:i]\n",
    "                    context += self.corpus[i+1:i+window_size+1]\n",
    "                elif len(self.corpus) - i <= window_size:\n",
    "                    context += self.corpus[i-window_size:i]\n",
    "                    context += self.corpus[i+1:]\n",
    "                else:\n",
    "                    context += self.corpus[i-window_size:i]\n",
    "                    context += self.corpus[i+1:i+window_size+1]\n",
    "                \n",
    "                # prediction\n",
    "                projection = self.W_in @ self.vocab[center_word] + self.b_in\n",
    "                softmax_result = self.softmax(self.W_out @ projection + self.b_out)\n",
    "                \n",
    "                #print(\"Context \", str(context))\n",
    "                context_error = 0\n",
    "                \n",
    "                for context_word in context:\n",
    "                    target = self.vocab[context_word]\n",
    "                    context_error += softmax_result - target\n",
    "                    epoch_loss += self.get_loss(softmax_result, target)\n",
    "                \n",
    "                self.W_in -= lr * (self.W_out.T @ context_error @ self.vocab[center_word].T)\n",
    "                self.b_in -= lr * (self.W_out.T @ context_error)\n",
    "                self.W_out -= lr * (context_error @ projection.T)\n",
    "                self.b_out -= lr * (context_error)\n",
    "            losses += [epoch_loss]    \n",
    "            #print('Losses: ', epoch_loss)\n",
    "        if return_history:\n",
    "            return losses\n",
    "            \n",
    "    def word2index(self, word):\n",
    "        \"\"\"\n",
    "        Returns index of the given word\n",
    "        \"\"\"\n",
    "        return int(np.where(self.vocab[word]==1)[0])\n",
    "    \n",
    "    def index2word(self, idx):\n",
    "        \"\"\"\n",
    "        Returns word that corresponds to the given index\n",
    "        \"\"\"\n",
    "        return sorted(self.vocab)[idx]\n",
    "    \n",
    "    \n",
    "    def get_embedding_dict(self):\n",
    "        \"\"\"\n",
    "        Returns dict with words in keys and their embeddings in values\n",
    "        \"\"\"\n",
    "        w2v_dict = {}\n",
    "        for i, key in enumerate(sorted(self.vocab)):\n",
    "            w2v_dict[key] = self.W_in[:,i]\n",
    "        return w2v_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = skip_gram(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Losses:  [[83819.81111759]]\n",
      "Epoch:  1\n",
      "Losses:  [[83772.44676828]]\n",
      "Epoch:  2\n",
      "Losses:  [[83726.84701367]]\n",
      "Epoch:  3\n",
      "Losses:  [[83683.06443065]]\n",
      "Epoch:  4\n",
      "Losses:  [[83641.1451391]]\n",
      "Epoch:  5\n",
      "Losses:  [[83601.12304729]]\n",
      "Epoch:  6\n",
      "Losses:  [[83563.01615982]]\n",
      "Epoch:  7\n",
      "Losses:  [[83526.8246292]]\n",
      "Epoch:  8\n",
      "Losses:  [[83492.52940001]]\n",
      "Epoch:  9\n",
      "Losses:  [[83460.09058463]]\n",
      "Epoch:  10\n",
      "Losses:  [[83429.44573484]]\n",
      "Epoch:  11\n",
      "Losses:  [[83400.50902427]]\n",
      "Epoch:  12\n",
      "Losses:  [[83373.17233965]]\n",
      "Epoch:  13\n",
      "Losses:  [[83347.30850347]]\n",
      "Epoch:  14\n",
      "Losses:  [[83322.77603316]]\n",
      "Epoch:  15\n",
      "Losses:  [[83299.42454207]]\n",
      "Epoch:  16\n",
      "Losses:  [[83277.1000792]]\n",
      "Epoch:  17\n",
      "Losses:  [[83255.65002297]]\n",
      "Epoch:  18\n",
      "Losses:  [[83234.9273172]]\n",
      "Epoch:  19\n",
      "Losses:  [[83214.79386868]]\n",
      "Epoch:  20\n",
      "Losses:  [[83195.12297507]]\n",
      "Epoch:  21\n",
      "Losses:  [[83175.80082337]]\n",
      "Epoch:  22\n",
      "Losses:  [[83156.72733324]]\n",
      "Epoch:  23\n",
      "Losses:  [[83137.816781]]\n",
      "Epoch:  24\n",
      "Losses:  [[83118.99865407]]\n",
      "Epoch:  25\n",
      "Losses:  [[83100.21904613]]\n",
      "Epoch:  26\n",
      "Losses:  [[83081.44259821]]\n",
      "Epoch:  27\n",
      "Losses:  [[83062.65454108]]\n",
      "Epoch:  28\n",
      "Losses:  [[83043.86196456]]\n",
      "Epoch:  29\n",
      "Losses:  [[83025.09329638]]\n",
      "Epoch:  30\n",
      "Losses:  [[83006.39530965]]\n",
      "Epoch:  31\n",
      "Losses:  [[82987.82777113]]\n",
      "Epoch:  32\n",
      "Losses:  [[82969.45677659]]\n",
      "Epoch:  33\n",
      "Losses:  [[82951.34833173]]\n",
      "Epoch:  34\n",
      "Losses:  [[82933.56340659]]\n",
      "Epoch:  35\n",
      "Losses:  [[82916.1547641]]\n",
      "Epoch:  36\n",
      "Losses:  [[82899.16511599]]\n",
      "Epoch:  37\n",
      "Losses:  [[82882.6260409]]\n",
      "Epoch:  38\n",
      "Losses:  [[82866.55736252]]\n",
      "Epoch:  39\n",
      "Losses:  [[82850.96691568]]\n",
      "Epoch:  40\n",
      "Losses:  [[82835.85071486]]\n",
      "Epoch:  41\n",
      "Losses:  [[82821.19358744]]\n",
      "Epoch:  42\n",
      "Losses:  [[82806.97036258]]\n",
      "Epoch:  43\n",
      "Losses:  [[82793.14758451]]\n",
      "Epoch:  44\n",
      "Losses:  [[82779.68542471]]\n",
      "Epoch:  45\n",
      "Losses:  [[82766.53929939]]\n",
      "Epoch:  46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-491-b2182e774b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-475-3c802456abe9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, window_size, epochs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mcontext_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msoftmax_result\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_in\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mcontext_error\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcenter_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-475-3c802456abe9>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#print(target @ pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m     \"\"\"\n\u001b[0;32m-> 2215\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gentype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m         \u001b[0;31m# 2018-02-25, 1.15.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m         warnings.warn(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sg.fit(epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
