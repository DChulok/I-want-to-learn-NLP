{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    with open(path, 'r') as f:\n",
    "        text_data = f.read()\n",
    "    for l in text_data.split('\\n'):\n",
    "        yield l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDF(path):\n",
    "    dicts = []\n",
    "    for d in parse(path):\n",
    "        #print(d)\n",
    "        try:\n",
    "            dicts.append(json.loads(d))\n",
    "        except:\n",
    "            print(d)\n",
    "            print(type(d))\n",
    "    return pd.DataFrame(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_normalizer(string):\n",
    "    strr = string.lower() #нижний регистр\n",
    "    strr = strr.strip() #выкидываем пробельные символы в начале и конце текста\n",
    "    strr = re.sub('\\s+', ' ', strr) #заменяем последовательности из пробельных символов на обычные пробелы\n",
    "    strr = re.sub(\"[^A-Za-z]+\", ' ', strr) #выкидываем не буквы \n",
    "    tok_str = word_tokenize(strr)\n",
    "    stopWords = set(stopwords.words('english')) #список стоп-слов\n",
    "    Lemmatizer = WordNetLemmatizer()\n",
    "    output_str = []\n",
    "    for word in tok_str:\n",
    "        if word not in stopWords: #выкидывае стоп-слова\n",
    "            output_str.append(Lemmatizer.lemmatize(word)) #добавляем лемматизированое слово\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "data = getDF('Data/Movies_and_TV_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADZPIG9QOCDG5</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>Alice L. Larson \"alice-loves-books\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is a charming version of the classic Dick...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>good version of a classic</td>\n",
       "      <td>1203984000</td>\n",
       "      <td>02 26, 2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A35947ZP82G7JH</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>Amarah Strack</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>It was good but not as emotionally moving as t...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Good but not as moving</td>\n",
       "      <td>1388361600</td>\n",
       "      <td>12 30, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3UORV8A9D5L2E</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Don't get me wrong, Winkler is a wonderful cha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Winkler's Performance was ok at best!</td>\n",
       "      <td>1388361600</td>\n",
       "      <td>12 30, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1VKW06X1O2X7V</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>Amazon Customer \"Softmill\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Henry Winkler is very good in this twist on th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It's an enjoyable twist on the classic story</td>\n",
       "      <td>1202860800</td>\n",
       "      <td>02 13, 2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>BABE</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This is one of the best Scrooge movies out.  H...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Best Scrooge yet</td>\n",
       "      <td>1387670400</td>\n",
       "      <td>12 22, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                         reviewerName helpful  \\\n",
       "0   ADZPIG9QOCDG5  0005019281  Alice L. Larson \"alice-loves-books\"  [0, 0]   \n",
       "1  A35947ZP82G7JH  0005019281                        Amarah Strack  [0, 0]   \n",
       "2  A3UORV8A9D5L2E  0005019281                      Amazon Customer  [0, 0]   \n",
       "3  A1VKW06X1O2X7V  0005019281           Amazon Customer \"Softmill\"  [0, 0]   \n",
       "4  A3R27T4HADWFFJ  0005019281                                 BABE  [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  This is a charming version of the classic Dick...      4.0   \n",
       "1  It was good but not as emotionally moving as t...      3.0   \n",
       "2  Don't get me wrong, Winkler is a wonderful cha...      3.0   \n",
       "3  Henry Winkler is very good in this twist on th...      5.0   \n",
       "4  This is one of the best Scrooge movies out.  H...      4.0   \n",
       "\n",
       "                                        summary  unixReviewTime   reviewTime  \n",
       "0                     good version of a classic      1203984000  02 26, 2008  \n",
       "1                        Good but not as moving      1388361600  12 30, 2013  \n",
       "2         Winkler's Performance was ok at best!      1388361600  12 30, 2013  \n",
       "3  It's an enjoyable twist on the classic story      1202860800  02 13, 2008  \n",
       "4                              Best Scrooge yet      1387670400  12 22, 2013  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a charming version of the classic Dicken\\'s tale.  Henry Winkler makes a good showing as the \"Scrooge\" character.  Even though you know what will happen this version has enough of a change to make it better that average.  If you love A Christmas Carol in any version, then you will love this.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "norm_rev = pd.Series(data['reviewText'].apply(my_normalizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('norm_texts.txt', 'w') as f:\n",
    "#    for x in norm_rev:\n",
    "#        f.write(' '.join(x) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_data = []\n",
    "#with open('norm_texts.txt', 'r') as f:\n",
    "#    for line in f:\n",
    "#        norm_data += [line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_df = pd.DataFrame(data={'review': [x.split() for x in norm_data]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_df = pd.DataFrame([], columns=['data'])\n",
    "#k = 0\n",
    "#with open('norm_texts.txt', 'r') as f:\n",
    "#    for line in f:\n",
    "#        norm_df.loc[k, 'data'] = [line.strip()]\n",
    "#        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation on train valid test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем 70% данных для тренировки и по 15% оставим на валидацию и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind, valid_ind, test_ind = np.split(np.random.permutation(norm_df.index), [int(len(norm_df)*0.7), int(len(norm_df)*0.85)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним индексы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('train_ind', train_ind)\n",
    "#np.save('valid_ind', valid_ind)\n",
    "#np.save('test_ind', test_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15000002945450838"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ind)/len(norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = norm_df.iloc[train_ind]\n",
    "#valid_df = norm_df.iloc[valid_ind]\n",
    "#test_df = norm_df.iloc[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv('train_df.csv')\n",
    "#valid_df.to_csv('valid_df.csv')\n",
    "#test_df.to_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilculkov/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "#train_df = pd.read_csv('train_df.csv', index_col=0)\n",
    "#valid_df = pd.read_csv('valid_df.csv', index_col=0)\n",
    "#test_df = pd.read_csv('test_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>855191</th>\n",
       "      <td>[movie, poor, quality, actor, never, heard, li...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473387</th>\n",
       "      <td>[great, group, costars, violent, child, great,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682386</th>\n",
       "      <td>[james, franco, may, resemble, james, dean, ho...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710604</th>\n",
       "      <td>[well, like, long, ago, old, chump, mine, deci...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715706</th>\n",
       "      <td>[review, series, apperently, made, rigid, fan,...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review  overall\n",
       "855191  [movie, poor, quality, actor, never, heard, li...      2.0\n",
       "473387  [great, group, costars, violent, child, great,...      4.0\n",
       "682386  [james, franco, may, resemble, james, dean, ho...      2.0\n",
       "710604  [well, like, long, ago, old, chump, mine, deci...      5.0\n",
       "715706  [review, series, apperently, made, rigid, fan,...      5.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim as gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем выделить некоторые очевидные биграмы в нашем тесте (аля Имя + Фамилия), для того чтобы сразу воспринимать их как некоторые единицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 33s, sys: 45.8 s, total: 7min 19s\n",
      "Wall time: 8min 34s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#phrases = Phrases(train_df['review'], min_count=int(len(train_df)*0.001))\n",
    "#bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмпиризм значения параметра min_count следующий: если биграм встречается (очень грубо говоря) хотя бы в 0.1% отзывов, то мы рассматриваем его всерьез. При выборе небольших значений этого параметра (+- 100) в него попадают словосочетания, которые на самом деле хотелось бы выявлять моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 13s, sys: 2min 17s, total: 6min 31s\n",
      "Wall time: 9min 26s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#train_df_with_bigram = train_df['review'].apply(lambda x: bigram[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilculkov/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#train_df['with_bigram'] = train_df_with_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.6 s, sys: 7.33 s, total: 55 s\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilculkov/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#valid_df['with_bigram']  = valid_df['review'].apply(lambda x: bigram[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilculkov/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "#train_df.drop(columns=['with_bigram'], inplace=True)\n",
    "#valid_df.drop(columns=['with_bigram'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__В последствии, я отказался от этой идеи, но все же стирать ничего не стал__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сначала натренить свою собственную модель word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(size=250, window=5, min_count=1, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 48.5 s, total: 3min\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.build_vocab(train_df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 1min 19s, sys: 1min 32s, total: 2h 2min 51s\n",
      "Wall time: 24min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1932365495, 2023813520)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_df['review'], total_examples=len(train_df), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мдааааа..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alcoholism', 0.5647395253181458),\n",
       " ('despair', 0.5455694794654846),\n",
       " ('poverty', 0.5399671792984009),\n",
       " ('loneliness', 0.5275277495384216),\n",
       " ('disillusionment', 0.5142602324485779),\n",
       " ('hopelessness', 0.5126552581787109),\n",
       " ('joblessness', 0.5081775784492493),\n",
       " ('prohibition', 0.48855435848236084),\n",
       " ('recession', 0.48155638575553894),\n",
       " ('malaise', 0.47719451785087585)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='depression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И правда ведь!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conventional', 0.677680253982544),\n",
       " ('traditionally', 0.5697474479675293),\n",
       " ('westernized', 0.5693082213401794),\n",
       " ('typical', 0.5363076329231262),\n",
       " ('classical', 0.5356478691101074),\n",
       " ('contemporary', 0.5137262344360352),\n",
       " ('modern', 0.5043244361877441),\n",
       " ('tradition', 0.5028184056282043),\n",
       " ('traditonal', 0.49846452474594116),\n",
       " ('style', 0.4758468568325043)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='traditional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('country', 0.6292040944099426),\n",
       " ('nation', 0.6078619956970215),\n",
       " ('american', 0.6031495332717896),\n",
       " ('usa', 0.599901556968689),\n",
       " ('korea', 0.5874038934707642),\n",
       " ('africa', 0.5859670042991638),\n",
       " ('europe', 0.5397226810455322),\n",
       " ('nanook', 0.519948422908783),\n",
       " ('japan', 0.5129011869430542),\n",
       " ('britain', 0.5075209736824036)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='america')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hour', 0.5217479467391968),\n",
       " ('minute', 0.5182461142539978),\n",
       " ('awhile', 0.5045113563537598),\n",
       " ('year', 0.49275457859039307),\n",
       " ('day', 0.49242666363716125),\n",
       " ('twice', 0.49231043457984924),\n",
       " ('since', 0.4696785509586334),\n",
       " ('week', 0.46965134143829346),\n",
       " ('movie', 0.460769921541214),\n",
       " ('month', 0.4499945640563965)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блин, так круто, аж наиграться не могу.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35250312"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('god', 'devil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И правда не очень похожи)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8487515"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('mother', 'father')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('father', 0.7148435115814209),\n",
       " ('dad', 0.6104788780212402),\n",
       " ('grandfather', 0.5882338285446167),\n",
       " ('son', 0.5732370615005493),\n",
       " ('uncle', 0.5580236911773682),\n",
       " ('brother', 0.4667826294898987),\n",
       " ('grandmother', 0.4646051526069641),\n",
       " ('grandpa', 0.46417585015296936),\n",
       " ('stepdad', 0.4605330228805542),\n",
       " ('mom', 0.45149534940719604)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['mother', 'man'], negative=['woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура! Вроде пока не очень-то все и плохо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраню ка я свою модельку, не зря же столько старалась!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.save_word2vec_format('model_1.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_1_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Simple RNN + our Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы попробуем построить простую RNN, используя уже полученные нашим ворд2веком (натренированным на train) эмбеддинги, для предсказания оценки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_model = Word2Vec.load(\"model_1_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(W2V_model.wv.vocab)\n",
    "embedding_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "for i in range(vocab_size):\n",
    "    embedding_vector = W2V_model.wv[W2V_model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим индексы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = np.load('train_ind.npy')\n",
    "valid_ind = np.load('valid_ind.npy')\n",
    "test_ind = np.load('test_ind.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1697533\n"
     ]
    }
   ],
   "source": [
    "# всего примеров\n",
    "print(len(train_ind) + len(valid_ind) + len(test_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilculkov/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "overall = pd.read_csv('overall.csv', index_col=0)\n",
    "overall['overall'] = overall['overall'].apply(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Немного отвлечемся..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного отвлечемся и посмотрим на наши данные, а именно на то, как они распределены относительно целевых классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall 1: 104219 samples = 6.139%\n",
      "Overall 2: 102410 samples = 6.033%\n",
      "Overall 3: 201302 samples = 11.859%\n",
      "Overall 4: 382994 samples = 22.562%\n",
      "Overall 5: 906608 samples = 53.407%\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print('Overall {}: {} samples = {}%'.format(i, np.sum(overall['overall']==i), round(np.sum(overall['overall']==i)*100/len(overall), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выходит, что в целом положительных отзывов в исходных данных значительно больше. Получается, что если бы мы всем предсказывали 5*, то получилась бы уже неплохая модель (шутка). Это не очень хорошо, ведь модель и правда может подогнаться под такой дисбалланс, что не даст нам требуемого результата, а будет лишь \"позитивно\" относиться ко всем отзывам (именно это у меня и получилось при первых попытках).\n",
    "\n",
    "Согласно гуглу, существует несколько основных подходов к работе с дисбаллансированными классами:\n",
    "\n",
    "1) Андерсэмплинг (придется что-то выкинуть)\n",
    "\n",
    "2) Оверсэмплинг (у нас и так больше миллиона отзывов в треине, а тут придется еще добивть)\n",
    "\n",
    "3) Навешивание весов классам в керасе (тут требуются определенные эвристики по выбору этих весов)\n",
    "\n",
    "Я решил остановиться на последнем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1188273"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#размер треина\n",
    "len(train_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для экспериментов по выбору весов классов попробуем оставить первые 50к отзывов, чтобы уменьшить время тренировки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train_ind = train_ind[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_train_ind = train_ind[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0607\n",
      "2 0.0608\n",
      "3 0.1164\n",
      "4 0.2223\n",
      "5 0.5398\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print(i, np.sum(overall.loc[tiny_train_ind, 'overall']==i)/len(tiny_train_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось вполне себе репрезентативненько."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее (немного заглядывая вперед, чтобы не загромождать notebook), для небольшого набора весов я немного потренил (ненастроенные) сети на мини треине и вот что получилось на валидации за 5 эпох:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_weights</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{0: 1, 1: 1, 2: 0.9, 3: 0.75, 4: 0.5}</td>\n",
       "      <td>1.129692</td>\n",
       "      <td>0.54498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{0: 1, 1: 1, 2: 0.9, 3: 0.8, 4: 0.6}</td>\n",
       "      <td>1.056613</td>\n",
       "      <td>0.56480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{0: 1, 1: 1, 2: 0.9, 3: 0.8, 4: 0.7}</td>\n",
       "      <td>1.013923</td>\n",
       "      <td>0.59174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{0: 1, 1: 1, 2: 0.9, 3: 0.85, 4: 0.75}</td>\n",
       "      <td>1.033361</td>\n",
       "      <td>0.57784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{0: 1, 1: 1, 2: 0.9, 3: 0.9, 4: 0.8}</td>\n",
       "      <td>1.046519</td>\n",
       "      <td>0.56732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{0: 1, 1: 1, 2: 0.95, 3: 0.95, 4: 0.9}</td>\n",
       "      <td>1.057140</td>\n",
       "      <td>0.56818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{0: 1, 1: 1, 2: 1, 3: 1, 4: 1}</td>\n",
       "      <td>1.099053</td>\n",
       "      <td>0.56232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            class_weights      loss  accuracy\n",
       "0   {0: 1, 1: 1, 2: 0.9, 3: 0.75, 4: 0.5}  1.129692   0.54498\n",
       "1    {0: 1, 1: 1, 2: 0.9, 3: 0.8, 4: 0.6}  1.056613   0.56480\n",
       "2    {0: 1, 1: 1, 2: 0.9, 3: 0.8, 4: 0.7}  1.013923   0.59174\n",
       "3  {0: 1, 1: 1, 2: 0.9, 3: 0.85, 4: 0.75}  1.033361   0.57784\n",
       "4    {0: 1, 1: 1, 2: 0.9, 3: 0.9, 4: 0.8}  1.046519   0.56732\n",
       "5  {0: 1, 1: 1, 2: 0.95, 3: 0.95, 4: 0.9}  1.057140   0.56818\n",
       "6          {0: 1, 1: 1, 2: 1, 3: 1, 4: 1}  1.099053   0.56232"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, оставляем вариант с наилучшим результатом хотя бы в таком приближении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1, 1: 1, 2: 0.9, 3: 0.8, 4: 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вернемся обратно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим данные. Функция чтения и кодирования нормализованного набора ревьюшек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_encoding_reviews(vocabulary, indexes_of_reviews):\n",
    "    encoded_reviews = []\n",
    "    target = []\n",
    "    with open('norm_texts.txt', 'r') as f:\n",
    "        k = 0\n",
    "        for line in f:\n",
    "            if k in indexes_of_reviews:\n",
    "                sen = line.split()\n",
    "                encoded_review = []\n",
    "                for x in sen:\n",
    "                    if x in vocabulary.keys():\n",
    "                        encoded_review += [vocabulary[x].index]\n",
    "                encoded_reviews.append(encoded_review)\n",
    "                target += [int(overall.loc[k, 'overall'])]\n",
    "            k+=1\n",
    "            if k % 250000 == 0:\n",
    "                print(\"Осталось прочитать:\", len(overall) - k)\n",
    "    return [encoded_reviews, target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем и кодируем треин"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Осталось прочитать: 1447533\n",
      "Осталось прочитать: 1197533\n",
      "Осталось прочитать: 947533\n",
      "Осталось прочитать: 697533\n",
      "Осталось прочитать: 447533\n",
      "Осталось прочитать: 197533\n",
      "CPU times: user 13min 13s, sys: 4.47 s, total: 13min 18s\n",
      "Wall time: 13min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "[encoded_train, train_y] = my_encoding_reviews(W2V_model.wv.vocab\n",
    "                                                    , train_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обрезаем ревьюшки по среднему размеру, так обучение проходит в десятки раз быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_len = 1\n",
    "sum_review_len = 0\n",
    "for x in encoded_train:\n",
    "    sum_review_len += len(x)\n",
    "    if len(x) > max_review_len:\n",
    "        max_review_len = len(x)\n",
    "avg_review_len = sum_review_len/len(encoded_train)\n",
    "padded_train = preprocessing.sequence.pad_sequences(encoded_train\n",
    "                                                    , maxlen = round(avg_review_len)\n",
    "                                                    , padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим непосредственно на саму нейронку. Я решил (следуя заданию) пока остановиться на простой двувунаправленной рекуррентной сети. Касательно числа нейронов в скрытых слоях: в случаях когда число нейронов скрытых слоев меньше - сети требуется искать меньше параметров, поэтому учится она по-быстрее, однако, мне кажется, немного опромедчиво делать небольшое число нейронов, ведь это наверняка лишь уменьшит гибкость сети. Но и очень много, наверное, тоже не нужно. Короче говоря, тут мне не совсем пока понятно, как нужно действовать. Оставлю пока 250 штук.В качестве оптимизирующего алгоритма берем 'adam' (в интернетах он пропогандируется как один из лучших). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 85, 250)           146464000 \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 146,717,005\n",
      "Trainable params: 253,005\n",
      "Non-trainable params: 146,464,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Linear stack of layers.\n",
    "RNN_model = keras.Sequential()\n",
    "RNN_model.add(layers.Embedding(input_dim=vocab_size\n",
    "                               , output_dim=embedding_size\n",
    "                               , input_length=round(avg_review_len)\n",
    "                               , weights=[embedding_matrix]\n",
    "                               , trainable=False\n",
    "                               , mask_zero=True))\n",
    "RNN_model.add(layers.Bidirectional(layers.SimpleRNN(units=250, activation='relu')))\n",
    "RNN_model.add(layers.Dense(units=5, activation='softmax'))\n",
    "RNN_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=0.5)\n",
    "                  , loss='sparse_categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При построении модели я столкнулся с казалось бы маленькой, но принеприятной для меня проблемой - аргументом mask_zero в слое Emdedding. Суть в том, что мой ворд2век присвоил этот индекс слову 'movie' и он как бы уже занят, однако заполнять здесь паддинги чем-то другим не выйдет, ибо аргумент mask_zero позволяет рассматривать лишь нули как пропуски. Чтобы справиться с этим, я пытался найти как можно избежать присвоения этого индекса ворд2веком, пытался добавить слой маскинг (который, как я понял, так сказать, не совместим со слоем Embedding), что в итоге ни к чему не привело. В конечно счете, я решил забить, ограничившись mask_zero=True (слово movie как бы все равно не имеет особого эмоционального окраса, так что пофиг)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бэтч-сайз я решил взять побольше, модель обучается так по-быстрее, да и поиск оптимума в целом более стабильный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 1188273 samples, validate on 50000 samples\n",
      "Epoch 1/5\n",
      "1188273/1188273 [==============================] - 1188s 999us/sample - loss: 0.6926 - accuracy: 0.6544 - val_loss: 0.6658 - val_accuracy: 0.6682\n",
      "Epoch 2/5\n",
      "1188273/1188273 [==============================] - 1081s 910us/sample - loss: 0.6863 - accuracy: 0.6574 - val_loss: 0.6661 - val_accuracy: 0.6667\n",
      "Epoch 3/5\n",
      "1188273/1188273 [==============================] - 1198s 1ms/sample - loss: 0.6806 - accuracy: 0.6603 - val_loss: 0.6644 - val_accuracy: 0.6701\n",
      "Epoch 4/5\n",
      "1188273/1188273 [==============================] - 1132s 952us/sample - loss: 0.6761 - accuracy: 0.6620 - val_loss: 0.6917 - val_accuracy: 0.6465\n",
      "Epoch 5/5\n",
      "1188273/1188273 [==============================] - 1169s 984us/sample - loss: 0.6718 - accuracy: 0.6639 - val_loss: 0.6737 - val_accuracy: 0.6691\n",
      "CPU times: user 9h 28min 16s, sys: 4h 25min 5s, total: 13h 53min 21s\n",
      "Wall time: 1h 36min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1af22e2850>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "RNN_model.fit(padded_train, np.asarray(train_y)-1\n",
    "              , epochs=5\n",
    "              , batch_size=1024\n",
    "              , verbose=1\n",
    "              , validation_data = (padded_valid[:50000], np.asarray(valid_y)[:50000]-1)\n",
    "              , class_weight=class_weights\n",
    "              , workers=10\n",
    "              , use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model.save_weights('Models/RNN_model_v1_relu_adam_0.001_10epochs', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1af2a9b290>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RNN_model.load_weights('Models/RNN_model_v1_tanh_adam_0.001_20epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что у нас будет на валидации. Оттуда выкинем незнакомые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Осталось прочитать: 1447533\n",
      "Осталось прочитать: 1197533\n",
      "Осталось прочитать: 947533\n",
      "Осталось прочитать: 697533\n",
      "Осталось прочитать: 447533\n",
      "Осталось прочитать: 197533\n",
      "CPU times: user 2min 20s, sys: 2.37 s, total: 2min 23s\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "[encoded_valid, valid_y] = my_encoding_reviews(W2V_model.wv.vocab, valid_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_valid = preprocessing.sequence.pad_sequences(encoded_valid\n",
    "                                                        , maxlen = round(avg_review_len)\n",
    "                                                        , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь я фиксировал результаты обучения\n",
    "results = pd.DataFrame([], columns=['description','epochs', 'train_loss', 'train_accuracy', 'valid_loss', 'valid_accuracy'])\n",
    "results.loc[len(results)] = ['tanh + sgd (lr=0.001)', 10, 0.8230, 0.5933, 1.0011,0.5854]\n",
    "results.loc[len(results)] = ['tanh + adam (lr=0.0005)', 10, 0.7345, 0.6344, 0.9041,0.6268]\n",
    "results.loc[len(results)] = ['tanh + adam (lr=0.0005)', 20, 0.7090, 0.6466, 0.7349,0.6309]\n",
    "results.loc[len(results)] = ['tanh + adam (lr=0.001)', 10, 0.7342, 0.6341, 0.8883,0.6306]\n",
    "results.loc[len(results)] = ['tanh + adam (lr=0.001)', 20, 0.7128, 0.6438, 0.8755,0.6373]\n",
    "results.loc[len(results)] = ['tanh + adam (lr=0.001)', 25, 0.7070, 0.6468, 0.8788,0.6356]\n",
    "results.loc[len(results)] = ['relu + adam (lr=0.0005)' ,10, 0.6828, 0.6603, 0.8461,0.6513]\n",
    "results.loc[len(results)] = ['relu + adam (lr=0.0005)', 20, 0.6319, 0.6810, 0.8652,0.6485]\n",
    "results.loc[len(results)] = ['relu + adam (lr=0.001)', 5, 0.6998, 0.6515, 0.8535,0.6469]\n",
    "results.loc[len(results)] = ['relu + adam (lr=0.001)', 10, 0.6718, 0.6639, 0.8590, 0.6471]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таблице ниже приводятся сравнительные результы моделей с разными функциями активации и лернинг рейтом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tanh + sgd (lr=0.001)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8230</td>\n",
       "      <td>0.5933</td>\n",
       "      <td>1.0011</td>\n",
       "      <td>0.5854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh + adam (lr=0.0005)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>0.6344</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>0.6268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tanh + adam (lr=0.0005)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.7090</td>\n",
       "      <td>0.6466</td>\n",
       "      <td>0.7349</td>\n",
       "      <td>0.6309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh + adam (lr=0.001)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7342</td>\n",
       "      <td>0.6341</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>0.6306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanh + adam (lr=0.001)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.6438</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>0.6373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tanh + adam (lr=0.001)</td>\n",
       "      <td>25</td>\n",
       "      <td>0.7070</td>\n",
       "      <td>0.6468</td>\n",
       "      <td>0.8788</td>\n",
       "      <td>0.6356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu + adam (lr=0.0005)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6828</td>\n",
       "      <td>0.6603</td>\n",
       "      <td>0.8461</td>\n",
       "      <td>0.6513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu + adam (lr=0.0005)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.6319</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>0.6485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>relu + adam (lr=0.001)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6998</td>\n",
       "      <td>0.6515</td>\n",
       "      <td>0.8535</td>\n",
       "      <td>0.6469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relu + adam (lr=0.001)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6718</td>\n",
       "      <td>0.6639</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.6471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               description epochs  train_loss  train_accuracy  valid_loss  \\\n",
       "0    tanh + sgd (lr=0.001)     10      0.8230          0.5933      1.0011   \n",
       "1  tanh + adam (lr=0.0005)     10      0.7345          0.6344      0.9041   \n",
       "2  tanh + adam (lr=0.0005)     20      0.7090          0.6466      0.7349   \n",
       "3   tanh + adam (lr=0.001)     10      0.7342          0.6341      0.8883   \n",
       "4   tanh + adam (lr=0.001)     20      0.7128          0.6438      0.8755   \n",
       "5   tanh + adam (lr=0.001)     25      0.7070          0.6468      0.8788   \n",
       "6  relu + adam (lr=0.0005)     10      0.6828          0.6603      0.8461   \n",
       "7  relu + adam (lr=0.0005)     20      0.6319          0.6810      0.8652   \n",
       "8   relu + adam (lr=0.001)      5      0.6998          0.6515      0.8535   \n",
       "9   relu + adam (lr=0.001)     10      0.6718          0.6639      0.8590   \n",
       "\n",
       "   valid_accuracy  \n",
       "0          0.5854  \n",
       "1          0.6268  \n",
       "2          0.6309  \n",
       "3          0.6306  \n",
       "4          0.6373  \n",
       "5          0.6356  \n",
       "6          0.6513  \n",
       "7          0.6485  \n",
       "8          0.6469  \n",
       "9          0.6471  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, в случае relu-активации уже после 10й эпохи качество модели на валидации перестает расти, соотствественно, пошло переобучение, причем сама модель \"выучилась\" быстрее всех. В случае с tanh-активацией аналогичное происходит после 20й эпохи. Примечательно, что выставление больших значений learning rate (0.002+) \"убивает\" модель во время обучения (проверено). Ограничение на норму градиента (клипнорм) также сильно помогло, без него модель ломается даже с lr=0.001 (и, возможно, меньшими рейтами). Последнее возможно обойти и без ограничения на норму - заменой оптимизатора на SGD, однако скорость обучения в таком случае сильно снижается (что также видно из таблицы), ну или нужно подобрать всякие параметры типа момента или подрубить Нестерова.\n",
    "\n",
    "__Итого__: лучшую оценку на валидаци получается модель с relu-активацией (хоть я ее и не хотел брать), оптимизированная при помощи adam с lr=0.001 на 10 эпохах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254630/254630 [==============================] - 130s 509us/sample - loss: 0.8590 - accuracy: 0.6471\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = RNN_model.evaluate(padded_valid, np.asarray(valid_y)-1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на матрицу ошибок лучшей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 41s, sys: 3min 42s, total: 16min 24s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_pred_ = RNN_model.predict(padded_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = np.argmax(valid_pred_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11145   1665    833    160   1803]\n",
      " [  4956   4231   3290    688   2077]\n",
      " [  2576   3377  11048   5506   7807]\n",
      " [  1453    893   6433  15118  33506]\n",
      " [  2077    438   2659   7661 123230]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(valid_y, valid_pred+1, labels=np.arange(1,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нууу, почти диагонально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля классификацей с ошибкой оценки не более чем на 1: 90.79 %\n"
     ]
    }
   ],
   "source": [
    "foo = 0\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if np.abs(i-j)>1:\n",
    "            foo += confusion_matrix(valid_y, valid_pred+1, labels=np.arange(1,6))[i][j]\n",
    "print(\"Доля классификацей с ошибкой оценки не более чем на 1: {} %\".format(round(100*(1-foo/len(valid_ind)), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.71      0.59     15606\n",
      "           2       0.40      0.28      0.33     15242\n",
      "           3       0.46      0.36      0.40     30314\n",
      "           4       0.52      0.26      0.35     57403\n",
      "           5       0.73      0.91      0.81    136065\n",
      "\n",
      "    accuracy                           0.65    254630\n",
      "   macro avg       0.52      0.51      0.50    254630\n",
      "weighted avg       0.62      0.65      0.62    254630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_y, valid_pred+1, labels=np.arange(1,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Simple RNN + Word2Vec from the Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь использовать взятый из Интернета натреннированный Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источник: [ссылка](https://code.google.com/archive/p/word2vec/) . Модель - __word2vec-google-news-300__.\n",
    "\n",
    "Также я хотел использовать модели с выглядящего надежнее [vectors.nlpl.eu](http://vectors.nlpl.eu/explore/embeddings/en/models/), но они там PoS-тэггированы, соотвественно, каждое слово имеет \"добавку\" вида 'table_NOUN'. Получается, нужна некоторая адаптация под наш случай, ибо одному слову (например, dance) могут соотвествовать несколько векторов и таким образом, надо их как-то усреднять чтоли или что-то такое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Google_W2V = models.KeyedVectors.load_word2vec_format('GoogleNews.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_google, embedding_size_google = Google_W2V.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_google = np.zeros((vocab_size_google, embedding_size_google))\n",
    "for i in range(vocab_size_google):\n",
    "    embedding_vector = Google_W2V[Google_W2V.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_google[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно по-новому закодировать треин:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Осталось прочитать: 1447533\n",
      "Осталось прочитать: 1197533\n",
      "Осталось прочитать: 947533\n",
      "Осталось прочитать: 697533\n",
      "Осталось прочитать: 447533\n",
      "Осталось прочитать: 197533\n",
      "CPU times: user 15min 25s, sys: 5.81 s, total: 15min 31s\n",
      "Wall time: 15min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "[encoded_train_google, train_y] = my_encoding_reviews(Google_W2V.vocab\n",
    "                                                    , train_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше все аналогично"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_len = 1\n",
    "sum_review_len = 0\n",
    "for x in encoded_train_google:\n",
    "    sum_review_len += len(x)\n",
    "    if len(x) > max_review_len:\n",
    "        max_review_len = len(x)\n",
    "avg_review_len = sum_review_len/len(encoded_train_google)\n",
    "padded_train_google = preprocessing.sequence.pad_sequences(encoded_train_google\n",
    "                                                    , maxlen = round(avg_review_len)\n",
    "                                                    , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_google = preprocessing.sequence.pad_sequences(encoded_train_google\n",
    "                                                        , maxlen = round(avg_review_len)\n",
    "                                                        , padding='post'\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели параметрами оставляем те же, что были у лучшей модели на нашем W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 82, 300)           900000000 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 500)               275500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 900,278,005\n",
      "Trainable params: 278,005\n",
      "Non-trainable params: 900,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Linear stack of layers.\n",
    "RNN_model_google = keras.Sequential()\n",
    "RNN_model_google.add(layers.Embedding(input_dim=vocab_size_google\n",
    "                               , output_dim=embedding_size_google\n",
    "                               , input_length=round(avg_review_len)\n",
    "                               , weights=[embedding_matrix_google]\n",
    "                               , trainable=False\n",
    "                               , mask_zero=True))\n",
    "RNN_model_google.add(layers.Bidirectional(layers.SimpleRNN(units=250, activation='relu')))\n",
    "RNN_model_google.add(layers.Dense(units=5, activation='softmax'))\n",
    "RNN_model_google.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001, clipnorm=0.5)\n",
    "                  , loss='sparse_categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "\n",
    "RNN_model_google.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пошла тренировочка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 1188273 samples, validate on 50000 samples\n",
      "Epoch 1/5\n",
      "1188273/1188273 [==============================] - 1159s 975us/sample - loss: 0.6900 - accuracy: 0.6551 - val_loss: 0.6799 - val_accuracy: 0.6629\n",
      "Epoch 2/5\n",
      "1188273/1188273 [==============================] - 1151s 969us/sample - loss: 0.6877 - accuracy: 0.6567 - val_loss: 0.6794 - val_accuracy: 0.6601\n",
      "Epoch 3/5\n",
      "1188273/1188273 [==============================] - 1147s 965us/sample - loss: 0.6770 - accuracy: 0.6615 - val_loss: 0.7255 - val_accuracy: 0.6293\n",
      "Epoch 4/5\n",
      "1188273/1188273 [==============================] - 1148s 966us/sample - loss: 0.6697 - accuracy: 0.6641 - val_loss: 0.6845 - val_accuracy: 0.6636\n",
      "Epoch 5/5\n",
      "1188273/1188273 [==============================] - 1147s 966us/sample - loss: 0.6639 - accuracy: 0.6666 - val_loss: 0.6878 - val_accuracy: 0.6551\n",
      "CPU times: user 9h 43min 49s, sys: 4h 35min, total: 14h 18min 50s\n",
      "Wall time: 1h 35min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d21278a90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "RNN_model_google.fit(padded_train_google, np.asarray(train_y)-1\n",
    "              , epochs=5\n",
    "              , batch_size=1024\n",
    "              , verbose=1\n",
    "              , validation_data = (padded_valid_google[:50000], np.asarray(valid_y)[:50000]-1)\n",
    "              , class_weight=class_weights\n",
    "              , workers=10\n",
    "              , use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model_google.save_weights('Models/RNN_model_google_relu_adam_0.001_10epochs', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1d219c8b90>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RNN_model_google.load_weights('Models/RNN_model_google_relu_adam_0.001_5epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фух. Ну, что-то кажется, мы не сильно лучше прошлой модель получили. Разве что обучение быстрее и сама модель более гибкая (в плане того, что слов то в этом W2V определенно больше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Осталось прочитать: 1447533\n",
      "Осталось прочитать: 1197533\n",
      "Осталось прочитать: 947533\n",
      "Осталось прочитать: 697533\n",
      "Осталось прочитать: 447533\n",
      "Осталось прочитать: 197533\n",
      "CPU times: user 2min 29s, sys: 6.48 s, total: 2min 36s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "[encoded_valid_google, valid_y] = my_encoding_reviews(Google_W2V.vocab, valid_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_valid_google = preprocessing.sequence.pad_sequences(encoded_valid_google\n",
    "                                                        , maxlen = round(avg_review_len)\n",
    "                                                        , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь я фиксировал результаты обучения\n",
    "results_google = pd.DataFrame([], columns=['description','epochs', 'train_loss', 'train_accuracy', 'valid_loss', 'valid_accuracy'])\n",
    "results_google.loc[len(results_google)] = ['relu + adam (lr=0.001)', 5, 0.7005, 0.6503, 0.8629,0.6446]\n",
    "results_google.loc[len(results_google)] = ['relu + adam (lr=0.001)', 10, 0.6639, 0.6666, 0.8706,0.6405]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu + adam (lr=0.001)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>0.6503</td>\n",
       "      <td>0.8629</td>\n",
       "      <td>0.6446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu + adam (lr=0.001)</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6639</td>\n",
       "      <td>0.6666</td>\n",
       "      <td>0.8706</td>\n",
       "      <td>0.6405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              description epochs  train_loss  train_accuracy  valid_loss  \\\n",
       "0  relu + adam (lr=0.001)      5      0.7005          0.6503      0.8629   \n",
       "1  relu + adam (lr=0.001)     10      0.6639          0.6666      0.8706   \n",
       "\n",
       "   valid_accuracy  \n",
       "0          0.6446  \n",
       "1          0.6405  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну, особо сильно чему-то новому модел за вторые 5 эпох не научилась. За исключением подгона под треин. Поэтому, остановимся на первой - 5 эпоховой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254630/254630 [==============================] - 177s 696us/sample - loss: 0.8629 - accuracy: 0.6446\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = RNN_model_google.evaluate(padded_valid_google, np.asarray(valid_y)-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 34s, sys: 8min 24s, total: 21min 58s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_pred_google_ = RNN_model_google.predict(padded_valid_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_google = np.argmax(valid_pred_google_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10806   1821    991    249   1739]\n",
      " [  4729   3996   3629    933   1955]\n",
      " [  2374   2960  11188   6766   7026]\n",
      " [  1337    805   6455  18140  30666]\n",
      " [  2169    486   2790  10608 120012]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(valid_y, valid_pred_google+1, labels=np.arange(1,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля классификацей с ошибкой оценки не более чем на 1: 91.02 %\n"
     ]
    }
   ],
   "source": [
    "foo = 0\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if np.abs(i-j)>1:\n",
    "            foo += confusion_matrix(valid_y, valid_pred_google+1, labels=np.arange(1,6))[i][j]\n",
    "print(\"Доля классификацей с ошибкой оценки не более чем на 1: {} %\".format(round(100*(1-foo/len(valid_ind)), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.69      0.58     15606\n",
      "           2       0.40      0.26      0.32     15242\n",
      "           3       0.45      0.37      0.40     30314\n",
      "           4       0.49      0.32      0.39     57403\n",
      "           5       0.74      0.88      0.81    136065\n",
      "\n",
      "    accuracy                           0.64    254630\n",
      "   macro avg       0.52      0.50      0.50    254630\n",
      "weighted avg       0.62      0.64      0.62    254630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_y, valid_pred_google+1, labels=np.arange(1,6)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
